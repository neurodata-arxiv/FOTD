<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">



  

<title type="text">Neurodata FOTD</title>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml" />
<link rel="alternate" type="text/html" href="http://localhost:4000/" />
<updated>2016-12-13T12:24:54-05:00</updated>
<id>http://localhost:4000/</id>
<author>
  <name>Neurodata</name>
  <uri>http://localhost:4000/</uri>
  <email>support@neurodata.io</email>
</author>


<entry>
  <title type="html"><![CDATA[ndstore ingest]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/tutorials/ndstore-ingest"/>
  <id>http://localhost:4000/tutorials/ndstore-ingest</id>
  <published>2016-12-09T00:00:00-05:00</published>
  <updated>2016-12-09T00:00:00-00:00</updated>
  
  <author>
    <name>Neurodata</name>
    <uri>http://localhost:4000</uri>
    <email>support@neurodata.io</email>
  </author>
  <category scheme="http://localhost:4000/tags/#ndstore" term="ndstore" /><category scheme="http://localhost:4000/tags/#data" term="data" />
  <content type="html">
  
    &lt;h1 id=&quot;ingesting-3d-data-volumes-with-annotations-in-hdf5-format-to-ndstore&quot;&gt;Ingesting 3D Data Volumes With Annotations in HDF5 Format to ndstore&lt;/h1&gt;

&lt;p&gt;In this tutorial we will show you how to take a 3D image volume and 3D annotation volume and: create a dataset, project, tokens and channels, ingest the data, and verify the process was successful.&lt;/p&gt;

&lt;h3 id=&quot;step-1-prepare-your-data&quot;&gt;Step 1: Prepare your data&lt;/h3&gt;

&lt;p&gt;First ensure that your data is written to a properly formatted hdf5 volume. As an example, &lt;a href=&quot;http://openconnecto.me/ocp/ca/kasthuri11/image/hdf5/3/1000,1300/2000,2200/1000,1200/&quot;&gt;this link&lt;/a&gt; provides a 300x200x200 tiff image of electron microscopy data, and &lt;a href=&quot;http://openconnecto.me/ocp/ca/kat11segments/annotation/hdf5/3/1000,1300/2000,2200/1000,1200/&quot;&gt;this link&lt;/a&gt; provides annotations for the same region. Shown below are slices of this image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://openconnecto.me/ocp/ca/kasthuri11/image/xy/3/1000,1300/2000,2200/1100/&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;http://openconnecto.me/ocp/ca/kat11segments/annotation/xy/3/1000,1300/2000,2200/1100/&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-2-interogating-your-data&quot;&gt;Step 2: Interogating your data&lt;/h3&gt;

&lt;p&gt;In order to create a dataset, project, and channels for your data, you first need to understand several details of your data. Some of them can be found by interogating the images, while others require insight into the data acquisition (such as resolution, for instance).&lt;/p&gt;

&lt;p&gt;The details which can be determined from your image are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;{x, y, z} image size&lt;/li&gt;
  &lt;li&gt;time range&lt;/li&gt;
  &lt;li&gt;data type&lt;/li&gt;
  &lt;li&gt;window range&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The details which require information about your particular data are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;dataset name (no spaces or special characters)&lt;/li&gt;
  &lt;li&gt;{x, y, z} offset&lt;/li&gt;
  &lt;li&gt;scaling levels&lt;/li&gt;
  &lt;li&gt;scaling option&lt;/li&gt;
  &lt;li&gt;{x, y, z} voxel resolution&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A description of each of these fields is available &lt;a href=&quot;http://docs.neurodata.io/ndstore/sphinx/datamodel.html#dataset-attributes&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In order to determine the details of interest from our data in HDF5 format, we will use Python’s &lt;code&gt;h5py&lt;/code&gt; library.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;h5py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;kasthuri11-image-hdf5-3-1000_1300-2000_2200-1000_1200-ocpcutout.h5&#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h5py&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;r&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;np_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;CUTOUT&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;X image size: &#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Y image size: &#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Z image size: &#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Time series: &#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Time range: (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d)&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Time range: (0, 0)&#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Data type: &#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Window range: (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f)&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p class=&quot;notice&quot;&gt;X image size:  200&lt;br /&gt;
Y image size:  200&lt;br /&gt;
Z image size:  300&lt;br /&gt;
Time series:  False&lt;br /&gt;
Time range: (0, 0)&lt;br /&gt;
Data type:  uint8&lt;br /&gt;
Window range: (0.000000, 254.000000)&lt;/p&gt;

&lt;p&gt;Summarizing these results and those that must be determined with more intimate knowledge of the data, we come up with the following table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;field&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;dataset name&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;neurostorm_kat11_scale3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;x size&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;200&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;y size&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;200&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;z size&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;time range&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(0, 0)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;data type&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;uint8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;window range&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;(0, 254)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;x offset&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;y offset&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;z offset&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;scaling levels&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;scaling option&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;z-slices&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;x voxel resolution&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;32 nm&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;y voxel resolution&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;32 nm&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;z voxel resolution&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;40 nm&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;step-3--create-a-dataset&quot;&gt;Step 3:  Create A Dataset&lt;/h3&gt;

&lt;p&gt;Once you have this metadata about your images, you can begin the process of creating a dataset.&lt;/p&gt;

&lt;p&gt;First, you should navigate to the server you wish to use (http://openconnecto.me/ocp/accounts/login/, for instance) and login (or register if you don’t have an account). Once you enter your login information, you will be able to select the &lt;code&gt;Datasets &amp;gt; Create New Dataset&lt;/code&gt; menu.&lt;/p&gt;


  
  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/tutorials/ndstore-ingest&quot;&gt;ndstore ingest&lt;/a&gt; was originally published by Neurodata at &lt;a href=&quot;http://localhost:4000&quot;&gt;Neurodata FOTD&lt;/a&gt; on December 09, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[neurostorm + neurostars]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/community/neurostars-participation"/>
  <id>http://localhost:4000/community/neurostars-participation</id>
  <published>2016-12-05T00:00:00-05:00</published>
  <updated>2016-12-05T00:00:00-00:00</updated>
  
  <author>
    <name>Neurodata</name>
    <uri>http://localhost:4000</uri>
    <email>support@neurodata.io</email>
  </author>
  <category scheme="http://localhost:4000/tags/#publicity" term="publicity" /><category scheme="http://localhost:4000/tags/#collaboration" term="collaboration" />
  <content type="html">
  
    &lt;h1 id=&quot;neurostars-as-a-platform-for-involvement&quot;&gt;NeuroStars as a platform for involvement&lt;/h1&gt;

&lt;p&gt;A key component to NeuroStorm is community involvement and engagement. We wish to work with neuroscientists, engineers, and data scientists, to maximize the interoperability of tools and encourage common priorities in research (such as, reproducibility, extensiblity).&lt;/p&gt;

&lt;p&gt;As such, we have created the &lt;code&gt;#neurostorm&lt;/code&gt; tag on &lt;a href=&quot;http://neurostars.org&quot;&gt;neurostars.org&lt;/a&gt;. We encourage users to ask questions, post suggestions, and generally get involved via this tag and will be monitoring it to ensure that everyone is being heard.&lt;/p&gt;

&lt;p&gt;Check out our &lt;a href=&quot;https://neurostars.org/t/neurostorm-proposing-a-global-platform-for-neuroscience/40&quot;&gt;first neurostarts post&lt;/a&gt; for more info!&lt;/p&gt;

  
  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/community/neurostars-participation&quot;&gt;neurostorm + neurostars&lt;/a&gt; was originally published by Neurodata at &lt;a href=&quot;http://localhost:4000&quot;&gt;Neurodata FOTD&lt;/a&gt; on December 05, 2016.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[To the cloud!]]></title>
  <link rel="alternate" type="text/html" href="http://localhost:4000/publications/to-the-cloud"/>
  <id>http://localhost:4000/publications/to-the-cloud</id>
  <published>2016-11-05T00:00:00-04:00</published>
  <updated>2016-12-05T00:00:00-00:00</updated>
  
  <author>
    <name>Neurodata</name>
    <uri>http://localhost:4000</uri>
    <email>support@neurodata.io</email>
  </author>
  <category scheme="http://localhost:4000/tags/#publicity" term="publicity" /><category scheme="http://localhost:4000/tags/#collaboration" term="collaboration" />
  <content type="html">
  
    &lt;h1 id=&quot;introducing-neurostorm&quot;&gt;Introducing NeuroStorm&lt;/h1&gt;

&lt;p&gt;A recent &lt;a href=&quot;http://www.cell.com/neuron/fulltext/S0896-6273(16)30783-8&quot;&gt;Neuron Neuroview&lt;/a&gt; (open-access link &lt;a href=&quot;https://github.com/jovo/cv/raw/master/ToTheCloud.pdf&quot;&gt;here&lt;/a&gt;) highlighted a new initiative to develop a platform for globalized neuroscience. This initiative aims to build a unified computational ecosystem to accelerate global neuroscience discovery, in particular, via easing the process linking data to models to experiments.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As the next generation of brain scientists grows up, we have an opportunity to provide them with a canvas on which they can craft ever more creative portraits of our minds.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This initiative was spawned from a series of workshops. In the first, &lt;a href=&quot;http://brainx.io&quot;&gt;Global Brain Workshop 2016&lt;/a&gt; hosted at Johns Hopkins University, participants worked together to highlight the three “Grand Challenges” of modern brain science, and suggest ways in which community members can get involved (available &lt;a href=&quot;https://arxiv.org/abs/1608.06548&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Our next steps include laying the groundwork for a centralized open-access cloud data storage platform for brain science data. We will then work with labs and organizations across North America and the World to ingest all known publicly available image datasets, ranging in scale from electron microscopy to MRI. With this powerful and accessible platform at our disposal, we will then work on educating the community with workshops about integrating their tools and workflows with this database. As the community of users grows, we will expand our development team and thereby capability to support more types of data and provide standardized computing solutions for researchers, as well.&lt;/p&gt;

&lt;p&gt;The best way to get involved is to send us an email at &lt;a href=&quot;mailto:support@neurodata.io&quot;&gt;support@neurodata.io&lt;/a&gt;, and subscribe to stay up to date with posts about news, tutorials, and other information.&lt;/p&gt;

&lt;p&gt;We’re excited to share our upcoming journey with you - it’s bound to be en-lightning (sorry, had to).&lt;/p&gt;

  
  &lt;p&gt;&lt;a href=&quot;http://localhost:4000/publications/to-the-cloud&quot;&gt;To the cloud!&lt;/a&gt; was originally published by Neurodata at &lt;a href=&quot;http://localhost:4000&quot;&gt;Neurodata FOTD&lt;/a&gt; on November 05, 2016.&lt;/p&gt;</content>
</entry>

</feed>
